"""
AFOLIE Image Size Nodes - å›¾åƒç¼©æ”¾èŠ‚ç‚¹
æä¾›åƒç´ å’Œå€æ•°ä¸¤ç§ç¼©æ”¾æ–¹å¼ï¼Œä»¥åŠåƒç´ å¯¹é½åŠŸèƒ½
"""

import os
import subprocess
import tempfile
import torch
import numpy as np
from PIL import Image, ImageDraw


def tensor2pil(image):
    """Convert tensor to PIL Image"""
    return Image.fromarray(np.clip(255. * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8))


def pil2tensor(image):
    """Convert PIL Image to tensor"""
    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)


# é‡‡æ ·æ–¹æ³•åˆ—è¡¨ï¼ˆä¸¤ä¸ªèŠ‚ç‚¹å…±ç”¨ï¼‰
SAMPLING_METHODS = [
    "ä¸¤æ¬¡ç«‹æ–¹(å¹³æ»‘æ¸å˜)",
    "ä¿ç•™ç»†èŠ‚(æ‰©å¤§)",
    "ä¿ç•™ç»†èŠ‚2.0",
    "ä¸¤æ¬¡ç«‹æ–¹(è¾ƒå¹³æ»‘)(æ‰©å¤§)",
    "ä¸¤æ¬¡ç«‹æ–¹(è¾ƒé”åˆ©)(ç¼©å‡)",
    "é‚»è¿‘(ç¡¬è¾¹ç¼˜)",
    "ä¸¤æ¬¡çº¿æ€§"
]


class AFOLIEå›¾åƒåƒç´ ç¼©æ”¾:
    """
    å›¾åƒåƒç´ ç¼©æ”¾èŠ‚ç‚¹
    é€šè¿‡æŒ‡å®šå®½åº¦å’Œé«˜åº¦æ¥è°ƒæ•´å›¾åƒå¤§å°
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "å®½åº¦": ("INT", {
                    "default": 512,
                    "min": 2,
                    "max": 2048,
                    "step": 1
                }),
                "é«˜åº¦": ("INT", {
                    "default": 512,
                    "min": 2,
                    "max": 2048,
                    "step": 1
                }),
                "é‡‡æ ·æ–¹æ³•": (SAMPLING_METHODS,),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "resize_image"
    CATEGORY = "AFOLIE/å›¾åƒ"

    def resize_image(self, å›¾åƒ, å®½åº¦, é«˜åº¦, é‡‡æ ·æ–¹æ³•):
        """
        ä½¿ç”¨åƒç´ å€¼è°ƒæ•´å›¾åƒå¤§å°
        
        Args:
            å›¾åƒ: è¾“å…¥å›¾åƒå¼ é‡
            å®½åº¦: ç›®æ ‡å®½åº¦ï¼ˆåƒç´ ï¼‰
            é«˜åº¦: ç›®æ ‡é«˜åº¦ï¼ˆåƒç´ ï¼‰
            é‡‡æ ·æ–¹æ³•: é‡æ–°é‡‡æ ·ç®—æ³•
        """
        batch_size = å›¾åƒ.shape[0]
        target_width = å®½åº¦
        target_height = é«˜åº¦
        
        # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªå›¾åƒ
        resized_images = []
        
        for i in range(batch_size):
            img = å›¾åƒ[i]
            
            # è½¬æ¢ä¸ºPILè¿›è¡Œé«˜è´¨é‡é‡é‡‡æ ·
            pil_img = tensor2pil(img)
            
            # å°†é‡é‡‡æ ·æ–¹æ³•æ˜ å°„åˆ°PILæ»¤é•œ
            resample_map = {
                "ä¸¤æ¬¡ç«‹æ–¹(å¹³æ»‘æ¸å˜)": Image.BICUBIC,
                "ä¿ç•™ç»†èŠ‚(æ‰©å¤§)": Image.LANCZOS,
                "ä¿ç•™ç»†èŠ‚2.0": Image.LANCZOS,
                "ä¸¤æ¬¡ç«‹æ–¹(è¾ƒå¹³æ»‘)(æ‰©å¤§)": Image.BICUBIC,
                "ä¸¤æ¬¡ç«‹æ–¹(è¾ƒé”åˆ©)(ç¼©å‡)": Image.BICUBIC,
                "é‚»è¿‘(ç¡¬è¾¹ç¼˜)": Image.NEAREST,
                "ä¸¤æ¬¡çº¿æ€§": Image.BILINEAR
            }
            
            pil_filter = resample_map.get(é‡‡æ ·æ–¹æ³•, Image.BICUBIC)
            
            # ä½¿ç”¨PILè°ƒæ•´å¤§å°
            resized_pil = pil_img.resize((target_width, target_height), pil_filter)
            
            # è½¬æ¢å›å¼ é‡
            resized_tensor = pil2tensor(resized_pil)
            
            resized_images.append(resized_tensor)
        
        # å°†æ‰€æœ‰å›¾åƒå †å å›æ‰¹æ¬¡
        result = torch.cat(resized_images, dim=0)
        
        return (result,)


class AFOLIEå›¾åƒå€æ•°ç¼©æ”¾:
    """
    å›¾åƒå€æ•°ç¼©æ”¾èŠ‚ç‚¹
    é€šè¿‡å€æ•°æ¥è°ƒæ•´å›¾åƒå¤§å°
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "å€æ•°": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.01,
                    "max": 12.0,
                    "step": 0.01
                }),
                "é‡‡æ ·æ–¹æ³•": (SAMPLING_METHODS,),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "resize_image"
    CATEGORY = "AFOLIE/å›¾åƒ"

    def resize_image(self, å›¾åƒ, å€æ•°, é‡‡æ ·æ–¹æ³•):
        """
        ä½¿ç”¨å€æ•°è°ƒæ•´å›¾åƒå¤§å°
        
        Args:
            å›¾åƒ: è¾“å…¥å›¾åƒå¼ é‡
            å€æ•°: ç¼©æ”¾å€æ•°
            é‡‡æ ·æ–¹æ³•: é‡æ–°é‡‡æ ·ç®—æ³•
        """
        batch_size, orig_height, orig_width, channels = å›¾åƒ.shape
        
        # æ ¹æ®å€æ•°è®¡ç®—ç›®æ ‡å°ºå¯¸
        target_width = int(orig_width * å€æ•°)
        target_height = int(orig_height * å€æ•°)
        
        # ç¡®ä¿å°ºå¯¸è‡³å°‘ä¸º1
        target_width = max(1, target_width)
        target_height = max(1, target_height)
        
        # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªå›¾åƒ
        resized_images = []
        
        for i in range(batch_size):
            img = å›¾åƒ[i]
            
            # è½¬æ¢ä¸ºPILè¿›è¡Œé«˜è´¨é‡é‡é‡‡æ ·
            pil_img = tensor2pil(img)
            
            # å°†é‡é‡‡æ ·æ–¹æ³•æ˜ å°„åˆ°PILæ»¤é•œ
            resample_map = {
                "ä¸¤æ¬¡ç«‹æ–¹(å¹³æ»‘æ¸å˜)": Image.BICUBIC,
                "ä¿ç•™ç»†èŠ‚(æ‰©å¤§)": Image.LANCZOS,
                "ä¿ç•™ç»†èŠ‚2.0": Image.LANCZOS,
                "ä¸¤æ¬¡ç«‹æ–¹(è¾ƒå¹³æ»‘)(æ‰©å¤§)": Image.BICUBIC,
                "ä¸¤æ¬¡ç«‹æ–¹(è¾ƒé”åˆ©)(ç¼©å‡)": Image.BICUBIC,
                "é‚»è¿‘(ç¡¬è¾¹ç¼˜)": Image.NEAREST,
                "ä¸¤æ¬¡çº¿æ€§": Image.BILINEAR
            }
            
            pil_filter = resample_map.get(é‡‡æ ·æ–¹æ³•, Image.BICUBIC)
            
            # ä½¿ç”¨PILè°ƒæ•´å¤§å°
            resized_pil = pil_img.resize((target_width, target_height), pil_filter)
            
            # è½¬æ¢å›å¼ é‡
            resized_tensor = pil2tensor(resized_pil)
            
            resized_images.append(resized_tensor)
        
        # å°†æ‰€æœ‰å›¾åƒå †å å›æ‰¹æ¬¡
        result = torch.cat(resized_images, dim=0)
        
        return (result,)


class AFOLIEå›¾åƒç½‘æ ¼è£å‰ª:
    """
    å›¾åƒç½‘æ ¼è£å‰ªèŠ‚ç‚¹
    æ ¹æ®æ¨ªå‘å’Œçºµå‘æ•°é‡å°†å›¾åƒå‡åŒ€è£å‰ªæˆå¤šä¸ªå­å›¾åƒ
    åŒæ—¶è¾“å‡ºå¸¦ç½‘æ ¼çº¿çš„é¢„è§ˆå›¾åƒ
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "æ¨ªå‘æ•°é‡": ("INT", {
                    "default": 2,
                    "min": 0,
                    "max": 20,
                    "step": 1,
                    "tooltip": "æ°´å¹³æ–¹å‘åˆ†å‰²æ•°é‡ï¼Œ0è¡¨ç¤ºä¸æ¨ªå‘è£å‰ª"
                }),
                "çºµå‘æ•°é‡": ("INT", {
                    "default": 2,
                    "min": 0,
                    "max": 20,
                    "step": 1,
                    "tooltip": "å‚ç›´æ–¹å‘åˆ†å‰²æ•°é‡ï¼Œ0è¡¨ç¤ºä¸çºµå‘è£å‰ª"
                }),
            },
        }

    RETURN_TYPES = ("IMAGE", "IMAGE")
    RETURN_NAMES = ("è£å‰ªå›¾åƒ", "é¢„è§ˆå›¾åƒ")
    FUNCTION = "crop_image"
    CATEGORY = "AFOLIE/å›¾åƒ"

    def crop_image(self, å›¾åƒ, æ¨ªå‘æ•°é‡, çºµå‘æ•°é‡):
        """
        å°†å›¾åƒæŒ‰ç½‘æ ¼è£å‰ªæˆå¤šä¸ªå­å›¾åƒï¼ŒåŒæ—¶ç”Ÿæˆé¢„è§ˆå›¾åƒ
        
        Args:
            å›¾åƒ: è¾“å…¥å›¾åƒå¼ é‡
            æ¨ªå‘æ•°é‡: æ°´å¹³æ–¹å‘åˆ†å‰²çš„å—æ•°ï¼ˆåˆ—æ•°ï¼‰ï¼Œ0è¡¨ç¤ºä¸æ¨ªå‘è£å‰ª
            çºµå‘æ•°é‡: å‚ç›´æ–¹å‘åˆ†å‰²çš„å—æ•°ï¼ˆè¡Œæ•°ï¼‰ï¼Œ0è¡¨ç¤ºä¸çºµå‘è£å‰ª
        
        Returns:
            è£å‰ªå›¾åƒ: è£å‰ªåçš„æ‰€æœ‰å­å›¾åƒï¼ˆæŒ‰ä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹çš„é¡ºåºï¼‰
            é¢„è§ˆå›¾åƒ: åŸå§‹å›¾åƒï¼ˆç”¨äºé¢„è§ˆï¼‰
        """
        batch_size, orig_height, orig_width, channels = å›¾åƒ.shape
        
        # å¤„ç† 0 çš„æƒ…å†µï¼š0 è¡¨ç¤ºä¸è£å‰ªè¯¥æ–¹å‘ï¼Œè§†ä¸º 1
        actual_æ¨ªå‘æ•°é‡ = max(1, æ¨ªå‘æ•°é‡)
        actual_çºµå‘æ•°é‡ = max(1, çºµå‘æ•°é‡)
        
        # è®¡ç®—æ¯ä¸ªå­å›¾å—çš„æ ‡å‡†å°ºå¯¸ï¼ˆå‘ä¸Šå–æ•´ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰åƒç´ ï¼‰
        block_width = (orig_width + actual_æ¨ªå‘æ•°é‡ - 1) // actual_æ¨ªå‘æ•°é‡
        block_height = (orig_height + actual_çºµå‘æ•°é‡ - 1) // actual_çºµå‘æ•°é‡
        
        # å­˜å‚¨æ‰€æœ‰è£å‰ªåçš„å›¾åƒå’Œé¢„è§ˆå›¾åƒ
        all_cropped_images = []
        preview_images = []
        
        # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªå›¾åƒ
        for b in range(batch_size):
            img = å›¾åƒ[b]
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            pil_img = tensor2pil(img)
            
            # === ç”Ÿæˆé¢„è§ˆå›¾åƒï¼ˆç›´æ¥ä½¿ç”¨åŸå›¾ï¼‰===
            preview_tensor = pil2tensor(pil_img)
            preview_images.append(preview_tensor)
            
            # === è£å‰ªå›¾åƒ ===
            # æŒ‰ç½‘æ ¼è£å‰ªï¼šä»ä¸Šåˆ°ä¸‹ï¼Œä»å·¦åˆ°å³
            for row in range(actual_çºµå‘æ•°é‡):
                for col in range(actual_æ¨ªå‘æ•°é‡):
                    # è®¡ç®—è£å‰ªåŒºåŸŸçš„åæ ‡
                    left = col * block_width
                    upper = row * block_height
                    right = min(left + block_width, orig_width)
                    lower = min(upper + block_height, orig_height)
                    
                    # è£å‰ªå›¾åƒ
                    cropped_pil = pil_img.crop((left, upper, right, lower))
                    
                    # å¦‚æœè£å‰ªåçš„å°ºå¯¸å°äºæ ‡å‡†å°ºå¯¸ï¼Œè°ƒæ•´åˆ°æ ‡å‡†å°ºå¯¸
                    # è¿™ç¡®ä¿æ‰€æœ‰å­å›¾åƒå°ºå¯¸ä¸€è‡´ï¼Œä¾¿äºåç»­æ‰¹é‡å¤„ç†
                    actual_width = right - left
                    actual_height = lower - upper
                    
                    if actual_width != block_width or actual_height != block_height:
                        # åˆ›å»ºæ ‡å‡†å°ºå¯¸çš„ç”»å¸ƒï¼Œå°†è£å‰ªçš„å›¾åƒæ”¾åœ¨å·¦ä¸Šè§’
                        # ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼è°ƒæ•´å°ºå¯¸ï¼Œä¿æŒåƒç´ è‰ºæœ¯çš„é”åˆ©è¾¹ç¼˜
                        cropped_pil = cropped_pil.resize(
                            (block_width, block_height), 
                            Image.NEAREST
                        )
                    
                    # è½¬æ¢å›å¼ é‡
                    cropped_tensor = pil2tensor(cropped_pil)
                    
                    all_cropped_images.append(cropped_tensor)
        
        # å°†æ‰€æœ‰è£å‰ªåçš„å›¾åƒå †å æˆæ‰¹æ¬¡
        cropped_result = torch.cat(all_cropped_images, dim=0)
        
        # å°†æ‰€æœ‰é¢„è§ˆå›¾åƒå †å æˆæ‰¹æ¬¡
        preview_result = torch.cat(preview_images, dim=0)
        
        return (cropped_result, preview_result)


class AFOLIEåƒç´ å¯¹é½:
    """
    åƒç´ å¯¹é½èŠ‚ç‚¹ - å°†åƒç´ å¯¹é½åˆ°å®Œç¾ç½‘æ ¼
    ä¿®å¤ AI ç”Ÿæˆçš„åƒç´ è‰ºæœ¯ä¸­çš„ä¸ä¸€è‡´é—®é¢˜
    """
    
    def __init__(self):
        # è·å–å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„ï¼ˆåœ¨ bin ç›®å½•ä¸‹ï¼‰
        current_dir = os.path.dirname(os.path.abspath(__file__))
        self.exe_path = os.path.join(current_dir, "bin", "spritefusion-pixel-snapper.exe")
        
        if not os.path.exists(self.exe_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°å¯æ‰§è¡Œæ–‡ä»¶: {self.exe_path}")
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "é¢œè‰²æ•°é‡": ("INT", {
                    "default": 16,
                    "min": 2,
                    "max": 256,
                    "step": 1,
                    "tooltip": "å›¾åƒå°†è¢«é‡åŒ–åˆ°è¿™ä¸ªæ•°é‡çš„é¢œè‰²"
                }),
            },
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("å¯¹é½åçš„å›¾åƒ",)
    FUNCTION = "process"
    CATEGORY = "AFOLIE/å›¾åƒ"
    DESCRIPTION = """
    å°†åƒç´ å¯¹é½åˆ°å®Œç¾ç½‘æ ¼
    
    åŠŸèƒ½ï¼š
    â€¢ å°†åƒç´ å¯¹é½åˆ°å®Œç¾ç½‘æ ¼
    â€¢ ä¿®å¤ AI ç”Ÿæˆåƒç´ è‰ºæœ¯çš„ä¸ä¸€è‡´
    â€¢ é‡åŒ–é¢œè‰²åˆ°ä¸¥æ ¼çš„è°ƒè‰²æ¿
    â€¢ ä¿æŒå°½å¯èƒ½å¤šçš„ç»†èŠ‚ï¼ˆå¦‚æŠ–åŠ¨ï¼‰
    
    é€‚ç”¨äºï¼š
    â€¢ AI ç”Ÿæˆçš„åƒç´ è‰ºæœ¯
    â€¢ ä¸é€‚åˆç½‘æ ¼çš„ç¨‹åºåŒ– 2D è‰ºæœ¯
    â€¢ éœ€è¦å®Œç¾ç¼©æ”¾çš„ 2D æ¸¸æˆèµ„æºå’Œ 3D çº¹ç†
    """
    
    def tensor_to_pil(self, tensor):
        """å°†å•ä¸ª ComfyUI å¼ é‡è½¬æ¢ä¸º PIL å›¾åƒ"""
        # tensor shape: [H, W, C]
        np_image = tensor.cpu().numpy()
        np_image = (np_image * 255).astype(np.uint8)
        pil_image = Image.fromarray(np_image)
        return pil_image
    
    def pil_to_tensor(self, pil_image):
        """å°† PIL å›¾åƒè½¬æ¢ä¸º ComfyUI å¼ é‡ï¼ˆæ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼‰"""
        np_image = np.array(pil_image).astype(np.float32)
        np_image = np_image / 255.0
        tensor = torch.from_numpy(np_image).unsqueeze(0)
        return tensor
    
    def process_single_image(self, pil_image, k_colors):
        """å¤„ç†å•å¼ å›¾åƒ"""
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as input_file:
            input_path = input_file.name
            pil_image.save(input_path, 'PNG')
        
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as output_file:
            output_path = output_file.name
        
        try:
            # æ„å»ºå‘½ä»¤
            cmd = [self.exe_path, input_path, output_path, str(k_colors)]
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode != 0:
                error_msg = result.stderr if result.stderr else "æœªçŸ¥é”™è¯¯"
                raise RuntimeError(f"åƒç´ å¯¹é½å¤„ç†å¤±è´¥: {error_msg}")
            
            # è¯»å–è¾“å‡ºå›¾åƒ
            output_image = Image.open(output_path)
            
            # ç¡®ä¿è¾“å‡ºå›¾åƒæ˜¯ RGB
            if output_image.mode not in ['RGB', 'RGBA']:
                output_image = output_image.convert('RGB')
            
            # å¦‚æœæ˜¯ RGBAï¼Œè½¬æ¢ä¸º RGB
            if output_image.mode == 'RGBA':
                background = Image.new('RGB', output_image.size, (255, 255, 255))
                background.paste(output_image, mask=output_image.split()[3])
                output_image = background
            
            return output_image
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(input_path)
            except:
                pass
            try:
                os.unlink(output_path)
            except:
                pass
    
    def process(self, å›¾åƒ, é¢œè‰²æ•°é‡):
        """å¤„ç†å›¾åƒï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼‰"""
        try:
            batch_size = å›¾åƒ.shape[0]
            output_tensors = []
            
            for i in range(batch_size):
                single_image = å›¾åƒ[i]
                original_height = single_image.shape[0]
                original_width = single_image.shape[1]
                
                pil_image = self.tensor_to_pil(single_image)
                output_image = self.process_single_image(pil_image, é¢œè‰²æ•°é‡)
                
                # å¦‚æœè¾“å‡ºå°ºå¯¸ä¸åŸå§‹å°ºå¯¸ä¸åŒï¼Œè°ƒæ•´å›åŸå§‹å°ºå¯¸
                if output_image.size != (original_width, original_height):
                    output_image = output_image.resize(
                        (original_width, original_height), 
                        Image.NEAREST
                    )
                
                output_tensor = self.pil_to_tensor(output_image)
                output_tensors.append(output_tensor)
            
            result = torch.cat(output_tensors, dim=0)
            return (result,)
                    
        except Exception as e:
            raise RuntimeError(f"åƒç´ å¯¹é½èŠ‚ç‚¹é”™è¯¯: {str(e)}")


# Node registration
NODE_CLASS_MAPPINGS = {
    "AFOLIEå›¾åƒåƒç´ ç¼©æ”¾": AFOLIEå›¾åƒåƒç´ ç¼©æ”¾,
    "AFOLIEå›¾åƒå€æ•°ç¼©æ”¾": AFOLIEå›¾åƒå€æ•°ç¼©æ”¾,
    "AFOLIEå›¾åƒç½‘æ ¼è£å‰ª": AFOLIEå›¾åƒç½‘æ ¼è£å‰ª,
    "AFOLIEåƒç´ å¯¹é½": AFOLIEåƒç´ å¯¹é½
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AFOLIEå›¾åƒåƒç´ ç¼©æ”¾": "å›¾åƒåƒç´ ç¼©æ”¾ ğŸ“",
    "AFOLIEå›¾åƒå€æ•°ç¼©æ”¾": "å›¾åƒå€æ•°ç¼©æ”¾ ğŸ”¢",
    "AFOLIEå›¾åƒç½‘æ ¼è£å‰ª": "å›¾åƒç½‘æ ¼è£å‰ª âœ‚ï¸",
    "AFOLIEåƒç´ å¯¹é½": "åƒç´ å¯¹é½ ğŸ¯"
}
