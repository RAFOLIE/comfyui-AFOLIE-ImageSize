"""
AFOLIE Inputæ‰¹æ¬¡å›¾åƒèŠ‚ç‚¹ - ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ‰¹æ¬¡å›¾åƒ
æä¾›ä¸‰ç§ä¸åŒçš„åŠ è½½æ–¹å¼
"""

import os
import torch
import numpy as np
from PIL import Image
import folder_paths


def pil2tensor(image):
    """Convert PIL Image to tensor"""
    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)


class AFOLIEInputæ‰¹æ¬¡å›¾åƒ:
    """
    Inputæ‰¹æ¬¡å›¾åƒèŠ‚ç‚¹ - å•å¼ è¾“å‡ºç‰ˆ
    ä»æŒ‡å®šæ–‡ä»¶å¤¹è·¯å¾„é€å¼ åŠ è½½å›¾åƒï¼ˆä¿æŒåŸå§‹å°ºå¯¸ï¼‰
    é…åˆComfyUIçš„æ‰¹å¤„ç†åŠŸèƒ½ï¼Œæ¯æ¬¡è¾“å‡ºä¸€å¼ å›¾åƒ
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "è·¯å¾„": ("STRING", {
                    "default": "E:/AI/ComfyUI_works/input_images",
                    "multiline": False
                }),
                "æ–‡ä»¶æ ¼å¼": (["all", "png", "jpg"],),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("å›¾åƒ",)
    FUNCTION = "load_images"
    CATEGORY = "AFOLIE/è¾“å…¥"
    OUTPUT_IS_LIST = (True,)

    def load_images(self, è·¯å¾„, æ–‡ä»¶æ ¼å¼="all"):
        """
        ä»æŒ‡å®šæ–‡ä»¶å¤¹é€å¼ åŠ è½½å›¾åƒï¼ˆä¿æŒåŸå§‹å°ºå¯¸ï¼‰
        è¿”å›å›¾åƒåˆ—è¡¨ï¼ŒComfyUIä¼šè‡ªåŠ¨è¿›è¡Œæ‰¹å¤„ç†
        """
        folder_path = è·¯å¾„.strip()
        if not folder_path or not os.path.exists(folder_path):
            print(f"è·¯å¾„ä¸å­˜åœ¨: {folder_path}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        # æ ¹æ®æ–‡ä»¶æ ¼å¼ç­›é€‰æ–‡ä»¶
        if æ–‡ä»¶æ ¼å¼ == "png":
            valid_extensions = ['.png']
        elif æ–‡ä»¶æ ¼å¼ == "jpg":
            valid_extensions = ['.jpg', '.jpeg']
        else:
            valid_extensions = ['.png', '.jpg', '.jpeg']
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        try:
            for filename in sorted(os.listdir(folder_path)):
                file_path = os.path.join(folder_path, filename)
                if os.path.isfile(file_path):
                    _, ext = os.path.splitext(filename.lower())
                    if ext in valid_extensions:
                        image_files.append(file_path)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤¹å¤±è´¥: {folder_path}, é”™è¯¯: {str(e)}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        if not image_files:
            print(f"æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å›¾åƒæ–‡ä»¶: {folder_path}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        # åŠ è½½æ‰€æœ‰å›¾åƒåˆ°åˆ—è¡¨
        loaded_images = []
        
        for idx, img_path in enumerate(image_files):
            try:
                pil_img = Image.open(img_path)
                _, ext = os.path.splitext(img_path.lower())
                
                # æ ¹æ®æ ¼å¼å¤„ç†å›¾åƒ
                if ext == '.png':
                    if pil_img.mode == 'RGBA':
                        pass
                    elif pil_img.mode == 'RGB':
                        pil_img = pil_img.convert('RGBA')
                    elif pil_img.mode in ('L', 'LA', 'P'):
                        pil_img = pil_img.convert('RGBA')
                    else:
                        pil_img = pil_img.convert('RGBA')
                    
                elif ext in ['.jpg', '.jpeg']:
                    if pil_img.mode == 'RGBA':
                        background = Image.new('RGB', pil_img.size, (255, 255, 255))
                        background.paste(pil_img, mask=pil_img.split()[3])
                        pil_img = background
                    elif pil_img.mode != 'RGB':
                        pil_img = pil_img.convert('RGB')
                    pil_img = pil_img.convert('RGBA')
                
                img_tensor = pil2tensor(pil_img)
                loaded_images.append(img_tensor)
                print(f"å·²åŠ è½½å›¾åƒ [{idx+1}/{len(image_files)}]: {os.path.basename(img_path)}")
                
            except Exception as e:
                print(f"åŠ è½½å›¾åƒå¤±è´¥: {img_path}, é”™è¯¯: {str(e)}")
                continue
        
        if not loaded_images:
            print("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾åƒ")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        image_count = len(loaded_images)
        print(f"æˆåŠŸåŠ è½½ {image_count} å¼ å›¾åƒï¼Œå°†é€å¼ è¾“å‡º")
        
        # è¿”å›å›¾åƒåˆ—è¡¨ï¼ŒComfyUIä¼šè‡ªåŠ¨æ‰¹å¤„ç†
        return (loaded_images,)


class AFOLIEInputæ‰¹æ¬¡å›¾åƒåƒç´ :
    """
    Inputæ‰¹æ¬¡å›¾åƒåƒç´ èŠ‚ç‚¹
    ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ‰¹æ¬¡å›¾åƒï¼Œå¹¶ç»Ÿä¸€è°ƒæ•´åˆ°æŒ‡å®šåƒç´ å°ºå¯¸
    é€å¼ è¾“å‡ºï¼Œé…åˆComfyUIçš„æ‰¹å¤„ç†åŠŸèƒ½
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "è·¯å¾„": ("STRING", {
                    "default": "E:/AI/ComfyUI_works/input_images",
                    "multiline": False
                }),
                "æ–‡ä»¶æ ¼å¼": (["all", "png", "jpg"],),
                "ç»Ÿä¸€å®½åº¦": ("INT", {
                    "default": 512,
                    "min": 64,
                    "max": 8192,
                    "step": 64
                }),
                "ç»Ÿä¸€é«˜åº¦": ("INT", {
                    "default": 512,
                    "min": 64,
                    "max": 8192,
                    "step": 64
                }),
                "é‡‡æ ·æ–¹æ³•": (["ä¿ç•™ç»†èŠ‚(Lanczos)", "ä¸¤æ¬¡ç«‹æ–¹(Bicubic)", "ä¸¤æ¬¡çº¿æ€§(Bilinear)", "é‚»è¿‘(Nearest)"],),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("å›¾åƒ",)
    FUNCTION = "load_images"
    CATEGORY = "AFOLIE/è¾“å…¥"
    OUTPUT_IS_LIST = (True,)

    def load_images(self, è·¯å¾„, æ–‡ä»¶æ ¼å¼="all", ç»Ÿä¸€å®½åº¦=512, ç»Ÿä¸€é«˜åº¦=512, é‡‡æ ·æ–¹æ³•="ä¿ç•™ç»†èŠ‚(Lanczos)"):
        """
        ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ‰¹æ¬¡å›¾åƒï¼Œå¹¶ç»Ÿä¸€è°ƒæ•´åˆ°æŒ‡å®šåƒç´ å°ºå¯¸
        """
        folder_path = è·¯å¾„.strip()
        if not folder_path or not os.path.exists(folder_path):
            print(f"è·¯å¾„ä¸å­˜åœ¨: {folder_path}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        # æ ¹æ®æ–‡ä»¶æ ¼å¼ç­›é€‰æ–‡ä»¶
        if æ–‡ä»¶æ ¼å¼ == "png":
            valid_extensions = ['.png']
        elif æ–‡ä»¶æ ¼å¼ == "jpg":
            valid_extensions = ['.jpg', '.jpeg']
        else:
            valid_extensions = ['.png', '.jpg', '.jpeg']
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        try:
            for filename in sorted(os.listdir(folder_path)):
                file_path = os.path.join(folder_path, filename)
                if os.path.isfile(file_path):
                    _, ext = os.path.splitext(filename.lower())
                    if ext in valid_extensions:
                        image_files.append(file_path)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤¹å¤±è´¥: {folder_path}, é”™è¯¯: {str(e)}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        if not image_files:
            print(f"æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å›¾åƒæ–‡ä»¶: {folder_path}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        # æ˜ å°„é‡‡æ ·æ–¹æ³•åˆ°PILå¸¸é‡
        resample_map = {
            "ä¿ç•™ç»†èŠ‚(Lanczos)": Image.LANCZOS,
            "ä¸¤æ¬¡ç«‹æ–¹(Bicubic)": Image.BICUBIC,
            "ä¸¤æ¬¡çº¿æ€§(Bilinear)": Image.BILINEAR,
            "é‚»è¿‘(Nearest)": Image.NEAREST
        }
        resample_filter = resample_map.get(é‡‡æ ·æ–¹æ³•, Image.LANCZOS)
        
        target_size = (ç»Ÿä¸€å®½åº¦, ç»Ÿä¸€é«˜åº¦)
        loaded_images = []
        resized_count = 0
        
        for idx, img_path in enumerate(image_files):
            try:
                pil_img = Image.open(img_path)
                original_size = pil_img.size
                _, ext = os.path.splitext(img_path.lower())
                
                # æ ¹æ®æ ¼å¼å¤„ç†å›¾åƒ
                if ext == '.png':
                    if pil_img.mode == 'RGBA':
                        pass
                    elif pil_img.mode == 'RGB':
                        pil_img = pil_img.convert('RGBA')
                    elif pil_img.mode in ('L', 'LA', 'P'):
                        pil_img = pil_img.convert('RGBA')
                    else:
                        pil_img = pil_img.convert('RGBA')
                    
                elif ext in ['.jpg', '.jpeg']:
                    if pil_img.mode == 'RGBA':
                        background = Image.new('RGB', pil_img.size, (255, 255, 255))
                        background.paste(pil_img, mask=pil_img.split()[3])
                        pil_img = background
                    elif pil_img.mode != 'RGB':
                        pil_img = pil_img.convert('RGB')
                    pil_img = pil_img.convert('RGBA')
                
                # è°ƒæ•´åˆ°ç»Ÿä¸€å°ºå¯¸
                # è°ƒæ•´åˆ°ç»Ÿä¸€å°ºå¯¸
                pil_img = pil_img.resize(target_size, resample_filter)
                if original_size != target_size:
                    resized_count += 1
                print(f"å·²åŠ è½½å›¾åƒ [{idx+1}/{len(image_files)}]: {os.path.basename(img_path)} ({original_size[0]}x{original_size[1]} -> {target_size[0]}x{target_size[1]})")
                
                img_tensor = pil2tensor(pil_img)
                loaded_images.append(img_tensor)
                
            except Exception as e:
                print(f"åŠ è½½å›¾åƒå¤±è´¥: {img_path}, é”™è¯¯: {str(e)}")
                continue
        
        if not loaded_images:
            print("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾åƒ")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        image_count = len(loaded_images)
        print(f"æˆåŠŸåŠ è½½ {image_count} å¼ å›¾åƒ (å…¶ä¸­ {resized_count} å¼ å·²è°ƒæ•´å°ºå¯¸åˆ° {target_size[0]}x{target_size[1]})ï¼Œå°†é€å¼ è¾“å‡º")
        
        return (loaded_images,)


class AFOLIEInputæ‰¹æ¬¡å›¾åƒå€æ•°:
    """
    Inputæ‰¹æ¬¡å›¾åƒå€æ•°èŠ‚ç‚¹
    ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ‰¹æ¬¡å›¾åƒï¼Œå¹¶æŒ‰å€æ•°ç»Ÿä¸€ç¼©æ”¾
    é€å¼ è¾“å‡ºï¼Œé…åˆComfyUIçš„æ‰¹å¤„ç†åŠŸèƒ½
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "è·¯å¾„": ("STRING", {
                    "default": "E:/AI/ComfyUI_works/input_images",
                    "multiline": False
                }),
                "æ–‡ä»¶æ ¼å¼": (["all", "png", "jpg"],),
                "å€æ•°": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.01,
                    "max": 12.0,
                    "step": 0.01
                }),
                "é‡‡æ ·æ–¹æ³•": (["ä¿ç•™ç»†èŠ‚(Lanczos)", "ä¸¤æ¬¡ç«‹æ–¹(Bicubic)", "ä¸¤æ¬¡çº¿æ€§(Bilinear)", "é‚»è¿‘(Nearest)"],),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("å›¾åƒ",)
    FUNCTION = "load_images"
    CATEGORY = "AFOLIE/è¾“å…¥"
    OUTPUT_IS_LIST = (True,)

    def load_images(self, è·¯å¾„, æ–‡ä»¶æ ¼å¼="all", å€æ•°=1.0, é‡‡æ ·æ–¹æ³•="ä¿ç•™ç»†èŠ‚(Lanczos)"):
        """
        ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ‰¹æ¬¡å›¾åƒï¼Œå¹¶æŒ‰å€æ•°ç»Ÿä¸€ç¼©æ”¾
        æ‰€æœ‰å›¾åƒä¼šæŒ‰ç…§ç¬¬ä¸€å¼ å›¾åƒçš„å°ºå¯¸ä½œä¸ºåŸºå‡†è¿›è¡Œå€æ•°ç¼©æ”¾
        """
        folder_path = è·¯å¾„.strip()
        if not folder_path or not os.path.exists(folder_path):
            print(f"è·¯å¾„ä¸å­˜åœ¨: {folder_path}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        # æ ¹æ®æ–‡ä»¶æ ¼å¼ç­›é€‰æ–‡ä»¶
        if æ–‡ä»¶æ ¼å¼ == "png":
            valid_extensions = ['.png']
        elif æ–‡ä»¶æ ¼å¼ == "jpg":
            valid_extensions = ['.jpg', '.jpeg']
        else:
            valid_extensions = ['.png', '.jpg', '.jpeg']
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        try:
            for filename in sorted(os.listdir(folder_path)):
                file_path = os.path.join(folder_path, filename)
                if os.path.isfile(file_path):
                    _, ext = os.path.splitext(filename.lower())
                    if ext in valid_extensions:
                        image_files.append(file_path)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤¹å¤±è´¥: {folder_path}, é”™è¯¯: {str(e)}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        if not image_files:
            print(f"æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å›¾åƒæ–‡ä»¶: {folder_path}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        # æ˜ å°„é‡‡æ ·æ–¹æ³•åˆ°PILå¸¸é‡
        resample_map = {
            "ä¿ç•™ç»†èŠ‚(Lanczos)": Image.LANCZOS,
            "ä¸¤æ¬¡ç«‹æ–¹(Bicubic)": Image.BICUBIC,
            "ä¸¤æ¬¡çº¿æ€§(Bilinear)": Image.BILINEAR,
            "é‚»è¿‘(Nearest)": Image.NEAREST
        }
        resample_filter = resample_map.get(é‡‡æ ·æ–¹æ³•, Image.LANCZOS)
        
        # å…ˆåŠ è½½ç¬¬ä¸€å¼ å›¾åƒè·å–åŸºå‡†å°ºå¯¸
        first_img = Image.open(image_files[0])
        base_width, base_height = first_img.size
        target_width = int(base_width * å€æ•°)
        target_height = int(base_height * å€æ•°)
        target_width = max(1, target_width)
        target_height = max(1, target_height)
        target_size = (target_width, target_height)
        
        print(f"åŸºå‡†å°ºå¯¸: {base_width}x{base_height}, å€æ•°: {å€æ•°}, ç›®æ ‡å°ºå¯¸: {target_width}x{target_height}")
        
        loaded_images = []
        
        for idx, img_path in enumerate(image_files):
            try:
                pil_img = Image.open(img_path)
                original_size = pil_img.size
                _, ext = os.path.splitext(img_path.lower())
                
                # æ ¹æ®æ ¼å¼å¤„ç†å›¾åƒ
                if ext == '.png':
                    if pil_img.mode == 'RGBA':
                        pass
                    elif pil_img.mode == 'RGB':
                        pil_img = pil_img.convert('RGBA')
                    elif pil_img.mode in ('L', 'LA', 'P'):
                        pil_img = pil_img.convert('RGBA')
                    else:
                        pil_img = pil_img.convert('RGBA')
                    
                elif ext in ['.jpg', '.jpeg']:
                    if pil_img.mode == 'RGBA':
                        background = Image.new('RGB', pil_img.size, (255, 255, 255))
                        background.paste(pil_img, mask=pil_img.split()[3])
                        pil_img = background
                    elif pil_img.mode != 'RGB':
                        pil_img = pil_img.convert('RGB')
                    pil_img = pil_img.convert('RGBA')
                
                # è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
                pil_img = pil_img.resize(target_size, resample_filter)
                print(f"å·²åŠ è½½å›¾åƒ [{idx+1}/{len(image_files)}]: {os.path.basename(img_path)} ({original_size[0]}x{original_size[1]} -> {target_size[0]}x{target_size[1]})")
                
                img_tensor = pil2tensor(pil_img)
                loaded_images.append(img_tensor)
                
            except Exception as e:
                print(f"åŠ è½½å›¾åƒå¤±è´¥: {img_path}, é”™è¯¯: {str(e)}")
                continue
        
        if not loaded_images:
            print("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾åƒ")
            empty_image = torch.zeros((1, 64, 64, 3))
            return ([empty_image],)
        
        image_count = len(loaded_images)
        print(f"æˆåŠŸåŠ è½½ {image_count} å¼ å›¾åƒ (å…¨éƒ¨ç¼©æ”¾åˆ° {target_size[0]}x{target_size[1]})ï¼Œå°†é€å¼ è¾“å‡º")
        
        return (loaded_images,)


# Node registration
NODE_CLASS_MAPPINGS = {
    "AFOLIEInputæ‰¹æ¬¡å›¾åƒ": AFOLIEInputæ‰¹æ¬¡å›¾åƒ,
    "AFOLIEInputæ‰¹æ¬¡å›¾åƒåƒç´ ": AFOLIEInputæ‰¹æ¬¡å›¾åƒåƒç´ ,
    "AFOLIEInputæ‰¹æ¬¡å›¾åƒå€æ•°": AFOLIEInputæ‰¹æ¬¡å›¾åƒå€æ•°
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AFOLIEInputæ‰¹æ¬¡å›¾åƒ": "Inputæ‰¹æ¬¡å›¾åƒ ğŸ“",
    "AFOLIEInputæ‰¹æ¬¡å›¾åƒåƒç´ ": "Inputæ‰¹æ¬¡å›¾åƒåƒç´  ğŸ“",
    "AFOLIEInputæ‰¹æ¬¡å›¾åƒå€æ•°": "Inputæ‰¹æ¬¡å›¾åƒå€æ•° ğŸ”¢"
}
